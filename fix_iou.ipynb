{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, logging\n",
    "import bfio\n",
    "from bfio import BioReader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchvision.models as models\n",
    "from torch import Tensor\n",
    "from typing import Union\n",
    "import albumentations\n",
    "from torch.nn.modules.loss import _Loss as TorchLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalNorm(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            window_size: int = 129,\n",
    "            max_response: Union[int, float] = 6,\n",
    "    ):\n",
    "        assert window_size % 2 == 1, 'window_size must be an odd integer'\n",
    "\n",
    "        self.window_size: int = window_size\n",
    "        self.max_response: float = float(max_response)\n",
    "        self.pad = torchvision.transforms.Pad(window_size // 2 + 1, padding_mode='reflect')\n",
    "        # Mode can be 'test', 'train' or 'eval'.\n",
    "        self.mode: str = 'eval'\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return torch.clip(\n",
    "            self.local_response(self.pad(x)),\n",
    "            min=-self.max_response,\n",
    "            max=self.max_response,\n",
    "        )\n",
    "\n",
    "    def image_filter(self, image: Tensor) -> Tensor:\n",
    "        \"\"\" Use a box filter on a stack of images\n",
    "        This method applies a box filter to an image. The input is assumed to be a\n",
    "        4D array, and should be pre-padded. The output will be smaller by\n",
    "        window_size - 1 pixels in both width and height since this filter does not pad\n",
    "        the input to account for filtering.\n",
    "        \"\"\"\n",
    "        integral_image: Tensor = image.cumsum(dim=-1).cumsum(dim=-2)\n",
    "        return (\n",
    "                integral_image[..., :-self.window_size - 1, :-self.window_size - 1]\n",
    "                + integral_image[..., self.window_size:-1, self.window_size:-1]\n",
    "                - integral_image[..., self.window_size:-1, :-self.window_size - 1]\n",
    "                - integral_image[..., :-self.window_size - 1, self.window_size:-1]\n",
    "        )\n",
    "\n",
    "    def local_response(self, image: Tensor):\n",
    "        \"\"\" Regional normalization.\n",
    "        This method normalizes each pixel using the mean and standard deviation of\n",
    "        all pixels within the window_size. The window_size parameter should be\n",
    "        2 * radius + 1 of the desired region of pixels to normalize by. The image should\n",
    "        be padded by window_size // 2 on each side.\n",
    "        \"\"\"\n",
    "        local_mean: Tensor = self.image_filter(image) / (self.window_size ** 2)\n",
    "        local_mean_square: Tensor = self.image_filter(image.pow(2)) / (self.window_size ** 2)\n",
    "\n",
    "        # Use absolute difference because sometimes error causes negative values\n",
    "        local_std = torch.clip(\n",
    "            (local_mean_square - local_mean.pow(2)).abs().sqrt(),\n",
    "            min=1e-3,\n",
    "        )\n",
    "\n",
    "        min_i, max_i = self.window_size // 2, -self.window_size // 2 - 1\n",
    "        response = image[..., min_i:max_i, min_i:max_i]\n",
    "\n",
    "        return (response - local_mean) / local_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"/home/vihanimm/SegmentationModelToolkit/workdir/SMP_Pipeline/output_SMP/Unet-MCCLoss-resnet18-random-Adadelta-multilabel-HorizontalFlip_ShiftScaleRotate_PadIfNeeded_RandomCrop_GaussianNoise_Perspective_RandomBrightnessContrast_RandomGamma_Sharpen_Blur_MotionBlur-NA/\" + \\\n",
    "    \"checkpoint.pth\"\n",
    "# modelpath = \"output_SMP\" + \\\n",
    "# \"/Unet-MCCLoss-resnet18-random-Adadelta-binary-HorizontalFlip_ShiftScaleRotate_PadIfNeeded_RandomCrop_GaussianNoise_Perspective_RandomBrightnessContrast_RandomGamma_Sharpen_Blur_MotionBlur-NA/\" + \\\n",
    "# \"checkpoint.pth\"\n",
    "# imagepath = \"/home/vihanimm/SegmentationModelToolkit/Data/tif_data/nuclear/validation/image_subset/nuclear_validation_967.tif\"\n",
    "# labelpath = \"/home/vihanimm/SegmentationModelToolkit/Data/tif_data/nuclear/validation/groundtruth_centerbinary_subset/nuclear_validation_967.tif\"\n",
    "imagedir = \"/home/vihanimm/SegmentationModelToolkit/Data/tif_data/nuclear/validation/image_subset/\"\n",
    "labeldir = \"/home/vihanimm/SegmentationModelToolkit/Data/tif_data/nuclear/validation/groundtruth_centerbinary_subset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0227)\n",
      "tensor(0.0338)\n",
      "tensor(0.1729)\n",
      "tensor(0.1922)\n",
      "tensor(0.2006)\n",
      "tensor(0.2059)\n",
      "tensor(0.0837)\n",
      "tensor(0.2625)\n",
      "tensor(0.1012)\n",
      "tensor(0.1400)\n",
      "tensor(0.0529)\n",
      "tensor(0.1264)\n",
      "tensor(0.0307)\n",
      "tensor(0.3316)\n",
      "tensor(0.3377)\n",
      "tensor(0.3805)\n",
      "tensor(0.3086)\n",
      "tensor(0.2366)\n",
      "tensor(0.2819)\n",
      "tensor(0.2138)\n",
      "tensor(0.2661)\n",
      "tensor(0.2031)\n",
      "tensor(0.0638)\n",
      "tensor(0.0973)\n",
      "tensor(0.3307)\n",
      "tensor(0.3237)\n",
      "tensor(0.3235)\n",
      "tensor(0.2994)\n",
      "tensor(0.3202)\n",
      "tensor(0.3079)\n",
      "tensor(0.3413)\n",
      "tensor(0.2765)\n",
      "tensor(0.0543)\n",
      "tensor(0.3858)\n",
      "tensor(0.3682)\n",
      "tensor(0.4159)\n",
      "tensor(0.3694)\n",
      "tensor(0.2949)\n",
      "tensor(0.2292)\n",
      "tensor(0.3700)\n",
      "tensor(0.2915)\n",
      "tensor(0.1977)\n",
      "tensor(0.0474)\n",
      "tensor(0.1954)\n",
      "tensor(0.1887)\n",
      "tensor(0.3483)\n",
      "tensor(0.2096)\n",
      "tensor(0.2579)\n",
      "tensor(0.2474)\n",
      "tensor(0.1521)\n",
      "tensor(0.2582)\n",
      "tensor(0.3368)\n",
      "tensor(0.3396)\n",
      "tensor(0.3616)\n",
      "tensor(0.0424)\n",
      "tensor(0.3703)\n",
      "tensor(0.3607)\n",
      "tensor(0.3584)\n",
      "tensor(0.3603)\n",
      "tensor(0.3358)\n",
      "tensor(0.3812)\n",
      "tensor(0.3564)\n",
      "tensor(0.3763)\n",
      "tensor(0.3866)\n",
      "tensor(0.3409)\n",
      "tensor(0.1625)\n",
      "tensor(0.3540)\n",
      "tensor(0.3902)\n",
      "tensor(0.3862)\n",
      "tensor(0.3849)\n",
      "tensor(0.3665)\n",
      "tensor(0.2581)\n",
      "tensor(0.1277)\n",
      "tensor(0.3228)\n",
      "tensor(0.3491)\n",
      "tensor(0.3304)\n",
      "tensor(0.0401)\n",
      "tensor(0.3287)\n",
      "tensor(0.2117)\n",
      "tensor(0.2833)\n",
      "tensor(0.2406)\n",
      "tensor(0.2792)\n",
      "tensor(0.3268)\n",
      "tensor(0.3058)\n",
      "tensor(0.3826)\n",
      "tensor(0.3532)\n",
      "tensor(0.3613)\n",
      "tensor(0.2179)\n",
      "tensor(0.3187)\n",
      "tensor(0.3106)\n",
      "tensor(0.3506)\n",
      "tensor(0.3170)\n",
      "tensor(0.3344)\n",
      "tensor(0.3126)\n",
      "tensor(0.3205)\n",
      "tensor(0.3576)\n",
      "tensor(0.3311)\n",
      "tensor(0.3495)\n",
      "tensor(0.0501)\n",
      "tensor(0.3328)\n",
      "tensor(0.2427)\n",
      "tensor(0.0922)\n",
      "tensor(0.1490)\n",
      "tensor(0.2328)\n",
      "tensor(0.2391)\n",
      "tensor(0.3175)\n",
      "tensor(0.2977)\n",
      "tensor(0.3328)\n",
      "tensor(0.3319)\n",
      "tensor(0.2222)\n",
      "tensor(0.1591)\n"
     ]
    }
   ],
   "source": [
    "MODELS = {\n",
    "    'Unet': smp.Unet,\n",
    "    'UnetPlusPlus': smp.UnetPlusPlus,\n",
    "    'MAnet': smp.MAnet,\n",
    "    'Linknet': smp.Linknet,\n",
    "    'FPN': smp.FPN,\n",
    "    'PSPNet': smp.PSPNet,\n",
    "    'PAN': smp.PAN,\n",
    "    'DeepLabV3': smp.DeepLabV3,\n",
    "    'DeepLabV3Plus': smp.DeepLabV3Plus,\n",
    "}\n",
    "\n",
    "import torchvision\n",
    "modelparameters = torch.load(modelpath)\n",
    "model = MODELS[modelparameters[\"model_name\"]](\n",
    "    encoder_name=modelparameters[\"encoder_variant\"],\n",
    "    encoder_weights=modelparameters[\"encoder_weights\"],\n",
    "    in_channels=1,\n",
    "    activation='sigmoid'\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "imagedir_list = os.listdir(imagedir)\n",
    "labeldir_list = os.listdir(labeldir)\n",
    "\n",
    "iou_score_eqn = smp.utils.metrics.IoU(activation='sigmoid')\n",
    "\n",
    "for imagepath, labelpath in zip(imagedir_list, labeldir_list):\n",
    "    assert imagepath == labelpath\n",
    "    imagepath = os.path.join(imagedir, imagepath)\n",
    "    labelpath = os.path.join(labeldir, labelpath)\n",
    "\n",
    "    preprocessing = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),LocalNorm()])\n",
    "    augmentations = []\n",
    "    with BioReader(imagepath, backend=\"java\") as br_imgobj:\n",
    "        br_imgarr = br_imgobj[:]\n",
    "        br_imgarr = np.reshape(br_imgarr, (1, 256, 256))\n",
    "\n",
    "        transform = albumentations.Compose(augmentations)\n",
    "        sample = transform(image=br_imgarr)\n",
    "        \n",
    "        image = sample['image'].squeeze()\n",
    "        preprocessedimage = preprocessing(image).numpy()\n",
    "        br_tensor = torch.from_numpy(preprocessedimage).to('cpu').unsqueeze(0)\n",
    "        output_tensor = model.predict(br_tensor)\n",
    "        output_image = output_tensor.numpy().squeeze()\n",
    "        # print(output_image.shape)\n",
    "        \n",
    "        # print(np.min(output_image), np.max(output_image))\n",
    "        # plt.imshow(output_image)\n",
    "        # plt.show()\n",
    "\n",
    "    with BioReader(labelpath, backend=\"java\") as br_labobj:\n",
    "        br_labarr = br_labobj[:]\n",
    "        br_labarr[br_labarr > 0] = 1\n",
    "        predict_tensor = torch.from_numpy(br_labarr).to('cpu').unsqueeze(0)\n",
    "        # plt.imshow(br_labarr)\n",
    "        # plt.show()\n",
    "        \n",
    "\n",
    "    iou_score = iou_score_eqn.forward(y_pr = output_tensor, y_gt = predict_tensor)\n",
    "    print(iou_score)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
